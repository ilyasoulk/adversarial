The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Selected Tasks: ['humaneval']
Loading model in fp32
number of problems for this task is 164
  0%|          | 0/164 [00:00<?, ?it/s]  1%|          | 1/164 [00:07<21:06,  7.77s/it]  1%|          | 2/164 [00:20<28:06, 10.41s/it]  2%|▏         | 3/164 [00:21<16:31,  6.16s/it]  2%|▏         | 4/164 [00:22<11:23,  4.27s/it]  3%|▎         | 5/164 [00:24<08:40,  3.27s/it]  4%|▎         | 6/164 [00:25<07:00,  2.66s/it]  4%|▍         | 7/164 [00:30<08:36,  3.29s/it]  5%|▍         | 8/164 [00:30<06:29,  2.50s/it]  5%|▌         | 9/164 [00:32<05:31,  2.14s/it]  6%|▌         | 10/164 [00:34<05:55,  2.31s/it]  7%|▋         | 11/164 [00:43<10:37,  4.17s/it]  7%|▋         | 12/164 [00:44<08:13,  3.24s/it]  8%|▊         | 13/164 [00:45<06:26,  2.56s/it]  9%|▊         | 14/164 [00:46<05:05,  2.04s/it]  9%|▉         | 15/164 [00:46<04:05,  1.65s/it] 10%|▉         | 16/164 [00:47<03:23,  1.37s/it] 10%|█         | 17/164 [00:48<02:47,  1.14s/it] 11%|█         | 18/164 [00:54<06:11,  2.54s/it] 12%|█▏        | 19/164 [00:55<05:24,  2.24s/it] 12%|█▏        | 20/164 [00:57<04:55,  2.05s/it] 13%|█▎        | 21/164 [00:58<04:20,  1.82s/it] 13%|█▎        | 22/164 [01:00<04:15,  1.80s/it] 14%|█▍        | 23/164 [01:01<03:34,  1.52s/it] 15%|█▍        | 24/164 [01:01<02:43,  1.17s/it] 15%|█▌        | 25/164 [01:02<02:34,  1.11s/it] 16%|█▌        | 26/164 [01:05<03:32,  1.54s/it] 16%|█▋        | 27/164 [01:06<03:11,  1.40s/it] 17%|█▋        | 28/164 [01:06<02:46,  1.23s/it] 18%|█▊        | 29/164 [01:07<02:16,  1.01s/it] 18%|█▊        | 30/164 [01:08<02:08,  1.04it/s] 19%|█▉        | 31/164 [01:09<01:59,  1.12it/s] 20%|█▉        | 32/164 [01:11<03:18,  1.50s/it] 20%|██        | 33/164 [04:26<2:09:29, 59.31s/it] 21%|██        | 34/164 [04:29<1:32:01, 42.47s/it] 21%|██▏       | 35/164 [04:29<1:04:12, 29.87s/it] 22%|██▏       | 36/164 [04:30<44:53, 21.04s/it]   23%|██▎       | 37/164 [04:31<31:57, 15.10s/it] 23%|██▎       | 38/164 [04:34<24:09, 11.50s/it] 24%|██▍       | 39/164 [04:39<19:39,  9.44s/it] 24%|██▍       | 40/164 [04:44<17:02,  8.25s/it] 25%|██▌       | 41/164 [04:50<15:15,  7.45s/it] 25%|██▌       | 41/164 [06:07<18:22,  8.96s/it]
Traceback (most recent call last):
  File "/lre/home/ioulkadda/bigcode-evaluation-harness/main.py", line 414, in <module>
    main()
  File "/lre/home/ioulkadda/bigcode-evaluation-harness/main.py", line 398, in main
    results[task] = evaluator.evaluate(
                    ^^^^^^^^^^^^^^^^^^^
  File "/lre/home/ioulkadda/bigcode-evaluation-harness/bigcode_eval/evaluator.py", line 95, in evaluate
    generations, references = self.generate_text(task_name, intermediate_generations=intermediate_generations)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lre/home/ioulkadda/bigcode-evaluation-harness/bigcode_eval/evaluator.py", line 69, in generate_text
    generations = parallel_generations(
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/lre/home/ioulkadda/bigcode-evaluation-harness/bigcode_eval/generation.py", line 141, in parallel_generations
    generations = complete_code(
                  ^^^^^^^^^^^^^^
  File "/lre/home/ioulkadda/bigcode-evaluation-harness/bigcode_eval/utils.py", line 303, in complete_code
    generated_tokens = model.generate(
                       ^^^^^^^^^^^^^^^
  File "/lre/home/ioulkadda/adversarial/dpo/env/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/lre/home/ioulkadda/adversarial/dpo/env/lib/python3.12/site-packages/transformers/generation/utils.py", line 2215, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/lre/home/ioulkadda/adversarial/dpo/env/lib/python3.12/site-packages/transformers/generation/utils.py", line 3249, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Traceback (most recent call last):
  File "/lre/home/ioulkadda/adversarial/dpo/env/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/lre/home/ioulkadda/adversarial/dpo/env/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/lre/home/ioulkadda/adversarial/dpo/env/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1174, in launch_command
    simple_launcher(args)
  File "/lre/home/ioulkadda/adversarial/dpo/env/lib/python3.12/site-packages/accelerate/commands/launch.py", line 766, in simple_launcher
    process.wait()
  File "/usr/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/subprocess.py", line 2053, in _wait
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/subprocess.py", line 2011, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
